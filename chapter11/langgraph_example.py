import os
import json
import re
from typing import List, Dict, Any, Optional, TypedDict, Literal
from pathlib import Path

# PDFå¤„ç†åº“
import pdfplumber

# LangChain & LangGraph ç»„ä»¶
from langchain_classic.output_parsers import ResponseSchema, StructuredOutputParser

# LangGraph æ ¸å¿ƒç»„ä»¶
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from init_client import init_llm

llm = init_llm(0.1)
# --- 1. å®šä¹‰ Agent çš„çŠ¶æ€ ---
class AgentState(TypedDict):
    """å®šä¹‰åœ¨æ•´ä¸ªå›¾ä¸­æµè½¬çš„çŠ¶æ€"""
    pdf_path: str
    paper_text: str
    goals: List[str]
    analysis: Dict[str, Any]
    current_summary: str
    evaluation: Dict[str, Any]
    iterations: int
    max_iterations: int
    final_result: Optional[Dict[str, Any]]
    error_message: Optional[str]


# --- 2. å®šä¹‰å›¾çš„èŠ‚ç‚¹ (æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªæ‰§è¡Œæ­¥éª¤) ---

def parse_pdf_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹1: è§£æPDFæ–‡ä»¶"""
    print("ğŸ” èŠ‚ç‚¹: è§£æPDFæ–‡ä»¶...")
    pdf_path = state["pdf_path"]
    if not os.path.exists(pdf_path):
        return {"error_message": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}

    try:
        with pdfplumber.open(pdf_path) as pdf:
            text_content = []
            max_pages = min(len(pdf.pages), 10)
            for i in range(max_pages):
                page = pdf.pages[i]
                text = page.extract_text()
                if text:
                    text_content.append(text)
            paper_text = "\n\n".join(text_content)
            paper_text = re.sub(r'\s+', ' ', paper_text)
            print(f"âœ… PDFè§£æå®Œæˆï¼Œå…±æå– {len(paper_text)} å­—ç¬¦")
            return {"paper_text": paper_text}
    except Exception as e:
        return {"error_message": f"PDFè§£æå¤±è´¥: {e}"}


def analyze_paper_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹2: åˆ†æè®ºæ–‡å†…å®¹"""
    print("ğŸ” èŠ‚ç‚¹: åˆ†æè®ºæ–‡å†…å®¹...")
    paper_text = state["paper_text"]

    analysis_schemas = [
        ResponseSchema(name="title", description="è®ºæ–‡æ ‡é¢˜"),
        ResponseSchema(name="authors", description="è®ºæ–‡ä½œè€…åˆ—è¡¨"),
        ResponseSchema(name="abstract", description="è®ºæ–‡æ‘˜è¦"),
        ResponseSchema(name="key_findings", description="ä¸»è¦å‘ç°ï¼Œä»¥åˆ—è¡¨å½¢å¼å‘ˆç°"),
        ResponseSchema(name="methodology", description="ç ”ç©¶æ–¹æ³•ç®€è¿°"),
    ]
    analysis_parser = StructuredOutputParser.from_response_schemas(analysis_schemas)

    prompt = PromptTemplate(
        input_variables=["paper_text", "format_instructions"],
        template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç ”ç©¶è®ºæ–‡ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚
{format_instructions}

è®ºæ–‡å†…å®¹:
{paper_text}

è¯·ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®å®Œæ•´ã€‚
        """
    )
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ã€‚"),
        HumanMessage(content=prompt.format(
            paper_text=paper_text[:8000],  # é™åˆ¶é•¿åº¦
            format_instructions=analysis_parser.get_format_instructions()
        ))
    ]
    response = llm.invoke(messages)
    analysis = analysis_parser.parse(response.content)

    print("âœ… è®ºæ–‡åˆ†æå®Œæˆ")
    return {"analysis": analysis}


def generate_summary_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹3: ç”Ÿæˆåˆå§‹æ‘˜è¦"""
    print("ğŸ” èŠ‚ç‚¹: ç”Ÿæˆåˆå§‹æ‘˜è¦...")
    analysis = state["analysis"]
    goals = state["goals"]

    prompt = PromptTemplate(
        input_variables=["goals", "analysis"],
        template="""
åŸºäºä»¥ä¸‹ç ”ç©¶è®ºæ–‡åˆ†æï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´æ˜äº†çš„æ‘˜è¦ï¼Œæ»¡è¶³ä»¥ä¸‹ç›®æ ‡:
{goals}

è®ºæ–‡åˆ†æ:
{analysis}

æ‘˜è¦åº”è¯¥:
1. ç®€æ˜æ‰¼è¦ï¼Œä¸è¶…è¿‡200å­—
2. çªå‡ºç ”ç©¶çš„ä¸»è¦è´¡çŒ®
3. ä½¿ç”¨æ¸…æ™°æ˜“æ‡‚çš„è¯­è¨€
4. é¿å…æŠ€æœ¯æœ¯è¯­è¿‡å¤š

è¯·ç›´æ¥è¿”å›æ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
        """
    )

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯å†™ä½œä¸“å®¶ï¼Œæ“…é•¿å°†å¤æ‚ç ”ç©¶è½¬åŒ–ä¸ºç®€æ´æ˜“æ‡‚çš„æ‘˜è¦ã€‚"),
        HumanMessage(content=prompt.format(
            goals=", ".join(goals),
            analysis=json.dumps(analysis, ensure_ascii=False, indent=2)
        ))
    ]
    response = llm.invoke(messages)
    summary = response.content.strip()

    print("âœ… åˆå§‹æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    return {"current_summary": summary, "iterations": 1}


def evaluate_summary_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹4: è¯„ä¼°æ‘˜è¦è´¨é‡"""
    print("ğŸ” èŠ‚ç‚¹: è¯„ä¼°æ‘˜è¦è´¨é‡...")
    summary = state["current_summary"]
    analysis = state["analysis"]
    goals = state["goals"]

    eval_schemas = [
        ResponseSchema(name="meets_goals", description="æ‘˜è¦æ˜¯å¦æ»¡è¶³æ‰€æœ‰è®¾å®šç›®æ ‡ï¼Œå›ç­”'æ˜¯'æˆ–'å¦'"),
        ResponseSchema(name="accuracy_score", description="æ‘˜è¦å‡†ç¡®åº¦è¯„åˆ†ï¼Œ1-10"),
        ResponseSchema(name="clarity_score", description="æ‘˜è¦æ¸…æ™°åº¦è¯„åˆ†ï¼Œ1-10"),
        ResponseSchema(name="feedback", description="æ”¹è¿›å»ºè®®ï¼Œå¦‚æœä¸æ»¡è¶³ç›®æ ‡"),
    ]
    eval_parser = StructuredOutputParser.from_response_schemas(eval_schemas)

    prompt = PromptTemplate(
        input_variables=["goals", "analysis", "summary", "format_instructions"],
        template="""
è¯„ä¼°ä»¥ä¸‹ç ”ç©¶è®ºæ–‡æ‘˜è¦æ˜¯å¦æ»¡è¶³è®¾å®šçš„ç›®æ ‡:

ç›®æ ‡: {goals}

è®ºæ–‡åˆ†æ:
{analysis}

æ‘˜è¦:
{summary}

{format_instructions}

è¯·å®¢è§‚è¯„ä¼°æ‘˜è¦è´¨é‡ï¼Œå¹¶æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
        """
    )

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°ç ”ç©¶æ‘˜è¦çš„è´¨é‡ã€‚"),
        HumanMessage(content=prompt.format(
            goals=", ".join(goals),
            analysis=json.dumps(analysis, ensure_ascii=False, indent=2),
            summary=summary,
            format_instructions=eval_parser.get_format_instructions()
        ))
    ]
    response = llm.invoke(messages)
    evaluation = eval_parser.parse(response.content)

    print(f"âœ… è¯„ä¼°å®Œæˆ - æ»¡è¶³ç›®æ ‡: {evaluation.get('meets_goals', 'æœªçŸ¥')}")
    return {"evaluation": evaluation}


def improve_summary_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹5: æ”¹è¿›æ‘˜è¦"""
    print("ğŸ” èŠ‚ç‚¹: æ”¹è¿›æ‘˜è¦...")
    summary = state["current_summary"]
    feedback = state["evaluation"].get("feedback", "éœ€è¦æ”¹è¿›")
    goals = state["goals"]

    prompt = PromptTemplate(
        input_variables=["summary", "feedback", "goals"],
        template="""
æ ¹æ®ä»¥ä¸‹åé¦ˆæ”¹è¿›ç ”ç©¶è®ºæ–‡æ‘˜è¦:

å½“å‰æ‘˜è¦:
{summary}

æ”¹è¿›åé¦ˆ:
{feedback}

ç›®æ ‡è¦æ±‚:
{goals}

è¯·æä¾›æ”¹è¿›åçš„æ‘˜è¦ï¼Œè¦æ±‚:
1. ä¿æŒç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—
2. å……åˆ†è€ƒè™‘åé¦ˆæ„è§
3. ç¡®ä¿æ»¡è¶³æ‰€æœ‰ç›®æ ‡è¦æ±‚

ç›´æ¥è¿”å›æ”¹è¿›åçš„æ‘˜è¦ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
        """
    )

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯å†™ä½œæ”¹è¿›ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®åé¦ˆä¼˜åŒ–ç ”ç©¶æ‘˜è¦ã€‚"),
        HumanMessage(content=prompt.format(
            summary=summary,
            feedback=feedback,
            goals=", ".join(goals)
        ))
    ]
    response = llm.invoke(messages)
    improved_summary = response.content.strip()

    print("âœ… æ‘˜è¦æ”¹è¿›å®Œæˆ")
    # å¢åŠ è¿­ä»£æ¬¡æ•°
    return {"current_summary": improved_summary, "iterations": state["iterations"] + 1}


def save_results_node(state: AgentState) -> AgentState:
    """èŠ‚ç‚¹6: ä¿å­˜æœ€ç»ˆç»“æœ"""
    print("ğŸ” èŠ‚ç‚¹: ä¿å­˜æœ€ç»ˆç»“æœ...")
    pdf_path = state["pdf_path"]
    analysis = state["analysis"]
    summary = state["current_summary"]
    evaluation = state["evaluation"]

    pdf_name = Path(pdf_path).stem
    output_dir = Path("analysis_results_langgraph")
    output_dir.mkdir(exist_ok=True)

    report_path = output_dir / f"{pdf_name}_analysis_report.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=== è®ºæ–‡åˆ†ææŠ¥å‘Š (LangGraphç‰ˆæœ¬) ===\n\n")
        f.write(f"è®ºæ–‡æ ‡é¢˜: {analysis.get('title', 'æœªçŸ¥')}\n")
        f.write(f"ä½œè€…: {analysis.get('authors', 'æœªçŸ¥')}\n\n")
        f.write("=== åˆ†æç»“æœ ===\n")
        f.write(json.dumps(analysis, ensure_ascii=False, indent=2))
        f.write("\n\n=== æœ€ç»ˆæ‘˜è¦ ===\n")
        f.write(summary)
        f.write("\n\n=== æœ€ç»ˆè¯„ä¼° ===\n")
        f.write(json.dumps(evaluation, ensure_ascii=False, indent=2))

    print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    return {"final_result": {"analysis": analysis, "summary": summary, "evaluation": evaluation}}


# --- 3. å®šä¹‰å†³ç­–é€»è¾‘ (å†³å®šä¸‹ä¸€æ­¥èµ°å‘å“ªä¸ªèŠ‚ç‚¹) ---

def should_continue(state: AgentState) -> Literal["improve_summary", "save_results", "end"]:
    """å†³ç­–å‡½æ•°ï¼šæ ¹æ®è¯„ä¼°ç»“æœå’Œè¿­ä»£æ¬¡æ•°å†³å®šä¸‹ä¸€æ­¥"""
    evaluation = state["evaluation"]
    iterations = state["iterations"]
    max_iterations = state["max_iterations"]

    if evaluation.get("meets_goals", "").lower() == "æ˜¯":
        print("âœ… å†³ç­–: ç›®æ ‡å·²æ»¡è¶³ï¼Œå‡†å¤‡ä¿å­˜ç»“æœã€‚")
        return "save_results"
    elif iterations < max_iterations:
        print(f"ğŸ”„ å†³ç­–: ç›®æ ‡æœªæ»¡è¶³ï¼Œä½†æœªè¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°({iterations}/{max_iterations})ï¼Œç»§ç»­æ”¹è¿›ã€‚")
        return "improve_summary"
    else:
        print(f"âš ï¸ å†³ç­–: å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œç»“æŸæµç¨‹ã€‚")
        return "end"


# --- 4. æ„å»ºå’Œç¼–è¯‘å›¾ ---

def build_graph():
    """æ„å»ºLangGraphå·¥ä½œæµå›¾"""
    workflow = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("parse_pdf", parse_pdf_node)
    workflow.add_node("analyze_paper", analyze_paper_node)
    workflow.add_node("generate_summary", generate_summary_node)
    workflow.add_node("evaluate_summary", evaluate_summary_node)
    workflow.add_node("improve_summary", improve_summary_node)
    workflow.add_node("save_results", save_results_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("parse_pdf")

    # æ·»åŠ çº¿æ€§è¾¹
    workflow.add_edge("parse_pdf", "analyze_paper")
    workflow.add_edge("analyze_paper", "generate_summary")
    workflow.add_edge("generate_summary", "evaluate_summary")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼šä»è¯„ä¼°èŠ‚ç‚¹åˆ°å†³ç­–
    workflow.add_conditional_edges(
        "evaluate_summary",
        should_continue,
        {
            "improve_summary": "improve_summary",
            "save_results": "save_results",
            "end": END
        }
    )

    # æ·»åŠ å¾ªç¯è¾¹ï¼šä»æ”¹è¿›èŠ‚ç‚¹å›åˆ°è¯„ä¼°èŠ‚ç‚¹
    workflow.add_edge("improve_summary", "evaluate_summary")

    # æ·»åŠ ç»“æŸè¾¹
    workflow.add_edge("save_results", END)

    # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹æ¥ä¿å­˜çŠ¶æ€ï¼ˆå¯é€‰ï¼Œä½†å¯¹äºæŒä¹…åŒ–å’Œè°ƒè¯•å¾ˆæœ‰ç”¨ï¼‰
    memory = MemorySaver()

    # ç¼–è¯‘å›¾
    app = workflow.compile(checkpointer=memory)
    return app


# --- 5. å°è£…æˆä¸»ç±» ---

class LangGraphPaperAnalyzer:
    def __init__(self, max_iterations: int = 3):
        self.max_iterations = max_iterations
        self.app = build_graph()
        # å¯é€‰ï¼šå¯è§†åŒ–å›¾çš„ç»“æ„
        self.app.get_graph().print_ascii()

    def analyze(self, pdf_path: str, goals: List[str]) -> Dict[str, Any]:
        """å¯åŠ¨åˆ†ææµç¨‹"""
        print(f"\nğŸš€ å¯åŠ¨ LangGraph è®ºæ–‡åˆ†æå™¨...")
        print("=" * 60)

        initial_state = {
            "pdf_path": pdf_path,
            "paper_text": "",
            "goals": goals,
            "analysis": {},
            "current_summary": "",
            "evaluation": {},
            "iterations": 0,
            "max_iterations": self.max_iterations,
            "final_result": None,
            "error_message": None
        }

        # ä½¿ç”¨ thread_id æ¥è·Ÿè¸ªç‰¹å®šçš„å¯¹è¯/è¿è¡Œ
        config = {"configurable": {"thread_id": "paper-analysis-1"}}

        # è¿è¡Œå›¾ç›´åˆ°ç»“æŸ
        final_state = self.app.invoke(initial_state, config=config)

        if final_state.get("error_message"):
            print(f"\nâŒ æµç¨‹å‡ºé”™: {final_state['error_message']}")
            return {"success": False, "error": final_state["error_message"]}

        if final_state.get("final_result"):
            print("\nâœ… åˆ†ææµç¨‹æˆåŠŸå®Œæˆï¼")
            return {"success": True, "result": final_state["final_result"]}
        else:
            print("\nâš ï¸ æµç¨‹ç»“æŸï¼Œä½†æœªè¾¾åˆ°ç›®æ ‡ã€‚")
            return {"success": False, "result": final_state, "message": "æœªåœ¨æœ€å¤§è¿­ä»£æ¬¡æ•°å†…è¾¾æˆç›®æ ‡"}


# --- 6. ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # åˆ›å»ºåˆ†æå™¨
    analyzer = LangGraphPaperAnalyzer(max_iterations=3)

    # è®¾å®šç›®æ ‡
    goals = [
        "ç®€æ´æ˜äº†",
        "çªå‡ºç ”ç©¶è´¡çŒ®",
        "é€‚åˆéä¸“ä¸šè¯»è€…ç†è§£",
        "åŒ…å«å…³é”®å‘ç°",
        "ä¸è¶…è¿‡200å­—"
    ]

    # åˆ†æPDFè®ºæ–‡
    pdf_file_path = "åŸºäºå…³ç³»é©±åŠ¨å¤šæ¨¡æ€åµŒå…¥å¡‘å½¢çš„å›¾åƒæè¿°ç”Ÿæˆ.pdf"  # æ›¿æ¢ä¸ºä½ çš„PDFæ–‡ä»¶è·¯å¾„

    if os.path.exists(pdf_file_path):
        result = analyzer.analyze(pdf_file_path, goals)

        if result["success"]:
            print("\n" + "=" * 60)
            print("ğŸ“Š æœ€ç»ˆåˆ†æç»“æœ:")
            print("=" * 60)
            final_data = result["result"]
            print(f"\nğŸ“ æœ€ç»ˆæ‘˜è¦:\n{final_data['summary']}")

            eval_data = final_data['evaluation']
            print(f"\nğŸ“ˆ æœ€ç»ˆè¯„ä¼°:")
            print(f"   æ»¡è¶³ç›®æ ‡: {eval_data.get('meets_goals', 'æœªçŸ¥')}")
            print(f"   å‡†ç¡®åº¦: {eval_data.get('accuracy_score', 'N/A')}/10")
            print(f"   æ¸…æ™°åº¦: {eval_data.get('clarity_score', 'N/A')}/10")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥æˆ–æœªå®Œæˆ: {result.get('error', result.get('message'))}")
    else:
        print(f"âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_file_path}")