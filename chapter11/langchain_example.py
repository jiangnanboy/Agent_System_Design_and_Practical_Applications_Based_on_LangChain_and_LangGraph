import os
import json
import re
from typing import List, Dict, Any, ClassVar
from pathlib import Path

# PDFå¤„ç†åº“
import pdfplumber

# LangChainç»„ä»¶
from langchain_classic.output_parsers import ResponseSchema, StructuredOutputParser
from langchain_core.output_parsers import BaseOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

from init_client import init_llm

class PDFParser:
    """PDFè§£æå™¨ - ä½¿ç”¨RunnableLambdaåŒ…è£…"""

    def __init__(self):
        self.parser = RunnableLambda(self._parse_pdf)

    def _parse_pdf(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        pdf_path = inputs["pdf_path"]

        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

        print(f"ğŸ“„ å¼€å§‹è§£æPDFæ–‡ä»¶: {pdf_path}")

        try:
            with pdfplumber.open(pdf_path) as pdf:
                text_content = []

                # æå–å‰10é¡µå†…å®¹ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
                max_pages = min(len(pdf.pages), 10)
                for i in range(max_pages):
                    page = pdf.pages[i]
                    text = page.extract_text()
                    if text:
                        text_content.append(text)

                paper_text = "\n\n".join(text_content)

                # æ¸…ç†æ–‡æœ¬
                paper_text = re.sub(r'\s+', ' ', paper_text)
                paper_text = re.sub(r'[^\w\s\u4e00-\u9fff.,;:!?()[]{}"\'-]', '', paper_text)

                print(f"âœ… PDFè§£æå®Œæˆï¼Œå…±æå– {len(paper_text)} å­—ç¬¦")
                return {"paper_text": paper_text}

        except Exception as e:
            print(f"âŒ PDFè§£æå¤±è´¥: {e}")
            return {"paper_text": ""}


class PaperAnalysisOutputParser(BaseOutputParser[Dict[str, Any]]):
    """è®ºæ–‡åˆ†æè¾“å‡ºè§£æå™¨"""

    # å°† schemas å®šä¹‰ä¸ºç±»å˜é‡
    analysis_schemas: ClassVar[List[ResponseSchema]] = [
        ResponseSchema(name="title", description="è®ºæ–‡æ ‡é¢˜"),
        ResponseSchema(name="authors", description="è®ºæ–‡ä½œè€…åˆ—è¡¨"),
        ResponseSchema(name="abstract", description="è®ºæ–‡æ‘˜è¦"),
        ResponseSchema(name="key_findings", description="ä¸»è¦å‘ç°ï¼Œä»¥åˆ—è¡¨å½¢å¼å‘ˆç°"),
        ResponseSchema(name="methodology", description="ç ”ç©¶æ–¹æ³•ç®€è¿°"),
        ResponseSchema(name="limitations", description="ç ”ç©¶å±€é™æ€§"),
        ResponseSchema(name="future_work", description="æœªæ¥å·¥ä½œå»ºè®®")
    ]

    # ä½¿ç”¨ @property æŒ‰éœ€åˆ›å»ºè§£æå™¨å®ä¾‹
    @property
    def _parser(self) -> StructuredOutputParser:
        return StructuredOutputParser.from_response_schemas(self.analysis_schemas)

    def parse(self, text: str) -> Dict[str, Any]:
        try:
            return self._parser.parse(text)
        except Exception as e:
            print(f"âš ï¸ åˆ†æç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”: {e}")
            return {"raw_analysis": text}

    def get_format_instructions(self) -> str:
        return self._parser.get_format_instructions()


class SummaryEvaluationOutputParser(BaseOutputParser[Dict[str, Any]]):
    """æ‘˜è¦è¯„ä¼°è¾“å‡ºè§£æå™¨"""

    # å°† schemas å®šä¹‰ä¸ºç±»å˜é‡
    evaluation_schemas: ClassVar[List[ResponseSchema]] = [
        ResponseSchema(name="meets_goals", description="æ‘˜è¦æ˜¯å¦æ»¡è¶³æ‰€æœ‰è®¾å®šç›®æ ‡ï¼Œå›ç­”'æ˜¯'æˆ–'å¦'"),
        ResponseSchema(name="accuracy_score", description="æ‘˜è¦å‡†ç¡®åº¦è¯„åˆ†ï¼Œ1-10"),
        ResponseSchema(name="clarity_score", description="æ‘˜è¦æ¸…æ™°åº¦è¯„åˆ†ï¼Œ1-10"),
        ResponseSchema(name="completeness_score", description="æ‘˜è¦å®Œæ•´æ€§è¯„åˆ†ï¼Œ1-10"),
        ResponseSchema(name="feedback", description="æ”¹è¿›å»ºè®®ï¼Œå¦‚æœä¸æ»¡è¶³ç›®æ ‡")
    ]

    # ä½¿ç”¨ @property æŒ‰éœ€åˆ›å»ºè§£æå™¨å®ä¾‹
    @property
    def _parser(self) -> StructuredOutputParser:
        return StructuredOutputParser.from_response_schemas(self.evaluation_schemas)

    def parse(self, text: str) -> Dict[str, Any]:
        try:
            return self._parser.parse(text)
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼°ç»“æœè§£æå¤±è´¥: {e}")
            return {"error": str(e)}

    def get_format_instructions(self) -> str:
        return self._parser.get_format_instructions()


class IntelligentPaperAnalyzer:
    """æ™ºèƒ½è®ºæ–‡åˆ†æå™¨ - ä½¿ç”¨ç®¡é“æ“ä½œç¬¦æ„å»ºAgent"""

    def __init__(self, max_iterations: int = 3):
        """åˆå§‹åŒ–æ™ºèƒ½è®ºæ–‡åˆ†æå™¨"""
        self.llm = init_llm(
            temperature=0.1
        )
        self.max_iterations = max_iterations
        self.goals = []

        # åˆå§‹åŒ–ç»„ä»¶ - ç°åœ¨å¯ä»¥æ­£å¸¸å®ä¾‹åŒ–
        self.pdf_parser = PDFParser()
        self.analysis_parser = PaperAnalysisOutputParser()
        self.evaluation_parser = SummaryEvaluationOutputParser()

        # æ„å»ºåˆ†æç®¡é“
        self._build_analysis_pipeline()
        # æ„å»ºè¯„ä¼°ç®¡é“
        self._build_evaluation_pipeline()
        # æ„å»ºæ”¹è¿›ç®¡é“
        self._build_improvement_pipeline()

    def _build_analysis_pipeline(self):
        """æ„å»ºè®ºæ–‡åˆ†æç®¡é“"""
        # åˆ†ææç¤ºæ¨¡æ¿
        self.analysis_prompt = PromptTemplate(
            input_variables=["paper_text", "format_instructions"],
            template="""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç ”ç©¶è®ºæ–‡ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚
            
            {format_instructions}
            
            è®ºæ–‡å†…å®¹:
            {paper_text}
            
            è¯·ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®å®Œæ•´ã€‚
                        """
        )

        # æ„å»ºåˆ†æç®¡é“
        self.analysis_pipeline = (
                RunnablePassthrough.assign(
                    format_instructions=lambda _: self.analysis_parser.get_format_instructions()
                )
                | self.analysis_prompt
                | self.llm
                | RunnableLambda(lambda x: x.content)
                | self.analysis_parser
        )

    def _build_evaluation_pipeline(self):
        """æ„å»ºæ‘˜è¦è¯„ä¼°ç®¡é“"""
        # è¯„ä¼°æç¤ºæ¨¡æ¿
        self.evaluation_prompt = PromptTemplate(
            input_variables=["goals", "analysis", "summary", "format_instructions"],
            template="""
            è¯„ä¼°ä»¥ä¸‹ç ”ç©¶è®ºæ–‡æ‘˜è¦æ˜¯å¦æ»¡è¶³è®¾å®šçš„ç›®æ ‡:
            
            ç›®æ ‡: {goals}
            
            è®ºæ–‡åˆ†æ:
            {analysis}
            
            æ‘˜è¦:
            {summary}
            
            {format_instructions}
            
            è¯·å®¢è§‚è¯„ä¼°æ‘˜è¦è´¨é‡ï¼Œå¹¶æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
                        """
        )

        # æ„å»ºè¯„ä¼°ç®¡é“
        self.evaluation_pipeline = (
                RunnablePassthrough.assign(
                    format_instructions=lambda _: self.evaluation_parser.get_format_instructions()
                )
                | self.evaluation_prompt
                | self.llm
                | RunnableLambda(lambda x: x.content)
                | self.evaluation_parser
        )

    def _build_improvement_pipeline(self):
        """æ„å»ºæ‘˜è¦æ”¹è¿›ç®¡é“"""
        # æ”¹è¿›æç¤ºæ¨¡æ¿
        self.improvement_prompt = PromptTemplate(
            input_variables=["summary", "feedback", "goals"],
            template="""
            æ ¹æ®ä»¥ä¸‹åé¦ˆæ”¹è¿›ç ”ç©¶è®ºæ–‡æ‘˜è¦:
            
            å½“å‰æ‘˜è¦:
            {summary}
            
            æ”¹è¿›åé¦ˆ:
            {feedback}
            
            ç›®æ ‡è¦æ±‚:
            {goals}
            
            è¯·æä¾›æ”¹è¿›åçš„æ‘˜è¦ï¼Œè¦æ±‚:
            1. ä¿æŒç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—
            2. å……åˆ†è€ƒè™‘åé¦ˆæ„è§
            3. ç¡®ä¿æ»¡è¶³æ‰€æœ‰ç›®æ ‡è¦æ±‚
            
            ç›´æ¥è¿”å›æ”¹è¿›åçš„æ‘˜è¦ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
                        """
        )

        # æ„å»ºæ”¹è¿›ç®¡é“
        self.improvement_pipeline = (
                self.improvement_prompt
                | self.llm
                | RunnableLambda(lambda x: x.content.strip())
        )

    def set_goals(self, goals: List[str]) -> None:
        """è®¾å®šåˆ†æç›®æ ‡"""
        self.goals = [g.strip() for g in goals]
        print(f"ğŸ¯ åˆ†æç›®æ ‡å·²è®¾å®š: {', '.join(self.goals)}")

    def analyze_paper(self, pdf_path: str) -> Dict[str, Any]:
        """åˆ†æPDFè®ºæ–‡çš„ä¸»æµç¨‹"""
        if not self.goals:
            raise ValueError("è¯·å…ˆä½¿ç”¨set_goals()æ–¹æ³•è®¾å®šåˆ†æç›®æ ‡")

        print(f"\nğŸš€ å¼€å§‹åˆ†æPDFè®ºæ–‡: {pdf_path}")
        print("=" * 60)

        # ç¬¬ä¸€æ­¥ï¼šè§£æPDF
        parse_result = self.pdf_parser.parser.invoke({"pdf_path": pdf_path})
        paper_text = parse_result["paper_text"]

        if not paper_text.strip():
            return {"success": False, "error": "æ— æ³•è§£æPDFå†…å®¹"}

        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ç®¡é“åˆ†æè®ºæ–‡
        print("ğŸ” å¼€å§‹åˆ†æè®ºæ–‡å†…å®¹...")
        analysis = self.analysis_pipeline.invoke({
            "paper_text": paper_text[:8000]  # é™åˆ¶æ–‡æœ¬é•¿åº¦
        })

        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆåˆå§‹æ‘˜è¦
        print("ğŸ“ ç”Ÿæˆç ”ç©¶è®ºæ–‡æ‘˜è¦...")
        summary_pipeline = (
                PromptTemplate(
                    input_variables=["goals", "analysis"],
                    template="""
                    åŸºäºä»¥ä¸‹ç ”ç©¶è®ºæ–‡åˆ†æï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´æ˜äº†çš„æ‘˜è¦ï¼Œæ»¡è¶³ä»¥ä¸‹ç›®æ ‡:
                    {goals}
                    
                    è®ºæ–‡åˆ†æ:
                    {analysis}
                    
                    æ‘˜è¦åº”è¯¥:
                    1. ç®€æ˜æ‰¼è¦ï¼Œä¸è¶…è¿‡200å­—
                    2. çªå‡ºç ”ç©¶çš„ä¸»è¦è´¡çŒ®
                    3. ä½¿ç”¨æ¸…æ™°æ˜“æ‡‚çš„è¯­è¨€
                    4. é¿å…æŠ€æœ¯æœ¯è¯­è¿‡å¤š
                    
                    è¯·ç›´æ¥è¿”å›æ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
                                    """
                )
                | self.llm
                | RunnableLambda(lambda x: x.content.strip())
        )

        current_summary = summary_pipeline.invoke({
            "goals": ", ".join(self.goals),
            "analysis": json.dumps(analysis, ensure_ascii=False, indent=2)
        })

        # ç¬¬å››æ­¥ï¼šè¿­ä»£è¯„ä¼°å’Œæ”¹è¿›
        iteration = 0
        final_evaluation = None

        while iteration < self.max_iterations:
            print(f"\n--- ğŸ” è¯„ä¼°è¿­ä»£ {iteration + 1}/{self.max_iterations} ---")

            # ä½¿ç”¨è¯„ä¼°ç®¡é“
            evaluation = self.evaluation_pipeline.invoke({
                "goals": ", ".join(self.goals),
                "analysis": json.dumps(analysis, ensure_ascii=False, indent=2),
                "summary": current_summary
            })

            meets_goals = evaluation.get("meets_goals", "").lower() == "æ˜¯"
            final_evaluation = evaluation

            print(f"âœ… è¯„ä¼°å®Œæˆ - æ»¡è¶³ç›®æ ‡: {evaluation.get('meets_goals', 'æœªçŸ¥')}")
            print(f"   å‡†ç¡®åº¦: {evaluation.get('accuracy_score', 'N/A')}/10")
            print(f"   æ¸…æ™°åº¦: {evaluation.get('clarity_score', 'N/A')}/10")
            print(f"   å®Œæ•´æ€§: {evaluation.get('completeness_score', 'N/A')}/10")

            if meets_goals:
                print(f"\nâœ… æ‘˜è¦æ»¡è¶³æ‰€æœ‰ç›®æ ‡ï¼Œåˆ†æå®Œæˆ (è¿­ä»£æ¬¡æ•°: {iteration + 1})")
                break

            # å¦‚æœä¸æ»¡è¶³ç›®æ ‡ä¸”æœªè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåˆ™æ”¹è¿›æ‘˜è¦
            if iteration < self.max_iterations - 1:
                print("\nğŸ› ï¸ æ‘˜è¦ä¸æ»¡è¶³ç›®æ ‡ï¼Œè¿›è¡Œæ”¹è¿›...")
                current_summary = self.improvement_pipeline.invoke({
                    "summary": current_summary,
                    "feedback": evaluation.get("feedback", "éœ€è¦æ”¹è¿›"),
                    "goals": ", ".join(self.goals)
                })
                iteration += 1
            else:
                print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.max_iterations})ï¼Œè¿”å›å½“å‰ç»“æœ")
                break

        # ä¿å­˜ç»“æœ
        self._save_results(pdf_path, analysis, current_summary, final_evaluation)

        return {
            "success": True,
            "analysis": analysis,
            "summary": current_summary,
            "evaluation": final_evaluation,
            "iterations": iteration + 1
        }

    def _save_results(self, pdf_path: str, analysis: Dict, summary: str, evaluation: Dict) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        pdf_name = Path(pdf_path).stem
        output_dir = Path("analysis_results")
        output_dir.mkdir(exist_ok=True)

        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_path = output_dir / f"{pdf_name}_analysis_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("=== è®ºæ–‡åˆ†ææŠ¥å‘Š ===\n\n")
            f.write(f"è®ºæ–‡æ ‡é¢˜: {analysis.get('title', 'æœªçŸ¥')}\n")
            f.write(f"ä½œè€…: {analysis.get('authors', 'æœªçŸ¥')}\n\n")

            f.write("=== åˆ†æç»“æœ ===\n")
            f.write(json.dumps(analysis, ensure_ascii=False, indent=2))
            f.write("\n\n")

            f.write("=== æ‘˜è¦ ===\n")
            f.write(summary)
            f.write("\n\n")

            f.write("=== è¯„ä¼°ç»“æœ ===\n")
            f.write(json.dumps(evaluation, ensure_ascii=False, indent=2))

        print(f"\nğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ™ºèƒ½è®ºæ–‡åˆ†æå™¨
    analyzer = IntelligentPaperAnalyzer(max_iterations=3)

    # è®¾å®šåˆ†æç›®æ ‡
    analyzer.set_goals([
        "ç®€æ´æ˜äº†",
        "çªå‡ºç ”ç©¶è´¡çŒ®",
        "é€‚åˆéä¸“ä¸šè¯»è€…ç†è§£",
        "åŒ…å«å…³é”®å‘ç°",
        "ä¸è¶…è¿‡200å­—"
    ])

    # åˆ†æPDFè®ºæ–‡
    # è¯·å°†è·¯å¾„æ›¿æ¢ä¸ºå®é™…çš„PDFæ–‡ä»¶è·¯å¾„
    pdf_file_path = "åŸºäºå…³ç³»é©±åŠ¨å¤šæ¨¡æ€åµŒå…¥å¡‘å½¢çš„å›¾åƒæè¿°ç”Ÿæˆ.pdf"  # æ›¿æ¢ä¸ºä½ çš„PDFæ–‡ä»¶è·¯å¾„

    if os.path.exists(pdf_file_path):
        result = analyzer.analyze_paper(pdf_file_path)

        if result["success"]:
            print("\n" + "=" * 60)
            print("ğŸ“Š æœ€ç»ˆåˆ†æç»“æœ:")
            print("=" * 60)
            print(f"\nğŸ“ æœ€ç»ˆæ‘˜è¦:\n{result['summary']}")

            if "evaluation" in result and result["evaluation"]:
                eval_data = result["evaluation"]
                print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
                print(f"   æ»¡è¶³ç›®æ ‡: {eval_data.get('meets_goals', 'æœªçŸ¥')}")
                print(f"   å‡†ç¡®åº¦: {eval_data.get('accuracy_score', 'N/A')}/10")
                print(f"   æ¸…æ™°åº¦: {eval_data.get('clarity_score', 'N/A')}/10")
                print(f"   å®Œæ•´æ€§: {eval_data.get('completeness_score', 'N/A')}/10")

            print(f"\nğŸ”„ è¿­ä»£æ¬¡æ•°: {result['iterations']}")
        else:
            print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
    else:
        print(f"âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_file_path}")
        print("è¯·å°†PDFæ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹pdf_file_pathå˜é‡æŒ‡å‘æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„")